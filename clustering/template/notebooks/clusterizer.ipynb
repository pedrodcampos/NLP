{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect import getsourcefile\n",
    "import os.path as path, sys\n",
    "current_dir = path.dirname(path.abspath(getsourcefile(lambda:0)))\n",
    "sys.path.append( \"/\".join(current_dir.split('/')[:-3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus.reader import wordnet\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize, sent_tokenize\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn  \n",
    "from langdetect import detect , detect_langs\n",
    "import tqdm\n",
    "from stacey.path import get_output_file\n",
    "import stacey \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def tag_stopwords(stopwords):\n",
    "    return  [stemmer.stem(lemma.lemmatize(pos[0], get_wordnet_pos(pos[1]))).lower()+\"_\"+get_wordnet_pos(pos[1]) \n",
    "             for sample, position in stopwords \n",
    "             for idx, pos in enumerate(pos_tag(word_tokenize(sample)))\n",
    "             if idx == position and get_wordnet_pos(pos[1]) not in [wordnet.ADJ,wordnet.ADV]]\n",
    "\n",
    "\n",
    "def get_us_names():\n",
    "    df_names= pd.read_csv('us-names.txt', header=None)\n",
    "    return df_names[0].apply(lambda x: x.lower()).tolist()\n",
    "\n",
    "def get_stopwords(extra=[],exception_words=[]):\n",
    "    us_names = get_us_names()\n",
    "\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stopwords.extend(us_names)\n",
    "    stopwords.extend(extra)\n",
    "    for word in exception_words:\n",
    "        try:\n",
    "            stopwords.remove(word)\n",
    "        except:\n",
    "            pass\n",
    "    stopwords.sort()\n",
    "    return stopwords\n",
    "\n",
    "def remove_stopwords(words,stopwords):\n",
    "    if len(stopwords)>0 and len(words)>0:\n",
    "        return np.delete(words,np.where(np.isin(np.char.lower(words), stopwords)))\n",
    "    return words\n",
    "\n",
    "    \n",
    "def lemmatize(tagged_words,lowercase, min_len):\n",
    "    \n",
    "    for word, tag in tagged_words:\n",
    "        if word.isalpha() and len(word) >= min_len:\n",
    "            \n",
    "            wordnet_pos = get_wordnet_pos(tag)\n",
    "            \n",
    "            if wordnet_pos == wordnet.ADJ or wordnet_pos == wordnet.ADV:\n",
    "                continue\n",
    "            \n",
    "            lemmatized = lemma.lemmatize(word, wordnet_pos)\n",
    "            \n",
    "            if lowercase:\n",
    "                lemmatized =  lemmatized.lower()\n",
    "            yield (lemmatized, wordnet_pos)\n",
    "    \n",
    "def tag_and_stem(corpus, lowercase=True, min_len=3, stopwords=[], tagged_stopwords=[]):\n",
    "    sentences = sent_tokenize(corpus)\n",
    "    tokens = [word for sent in sentences for word in word_tokenize(sent)]\n",
    "    tokens = remove_stopwords(tokens, stopwords)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    tagged = [item for item in lemmatize(pos_tags, lowercase, min_len)]\n",
    "    stemmed = [stemmer.stem(word)+'_'+pos for word,pos in tagged if word.isalpha()]\n",
    "    stemmed = remove_stopwords(stemmed,tagged_stopwords)\n",
    "    return ' '.join(stemmed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(get_output_file('clean_aurea_smb_mails_essentials.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_names = []\n",
    "unique_names = df['Assignee Name'].unique()\n",
    "for full_name in unique_names:\n",
    "    for name in full_name.split():\n",
    "        if name.isalpha() and len(name)>2:\n",
    "            custom_names.append(name.lower())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "lemma = WordNetLemmatizer()\n",
    "extra = ['xls', 'xlsx', 'google']+custom_names\n",
    "stopwords = stacey.stopwords.get_stopwords(extra)\n",
    "\n",
    "word_samples= [('please',0), ('thanks',0), ('email',0), ('timestamp',0), \n",
    "               ('today',0), ('team',0), ('one',0), ('Please do it',0), \n",
    "               ('could',0), ('hello',0), ('group',0), ('let me know',0),('let me know',2),\n",
    "              ('address', 0), ('concern',0), ('question',0), ('aircall',0)]\n",
    "               \n",
    "              \n",
    "tagged_stopwords = tag_stopwords(word_samples)+[]\n",
    "print(tagged_stopwords,extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stemmed_comments'] = df['clean_comments'].progress_apply(lambda x: tag_and_stem(x,stopwords=stopwords, tagged_stopwords=tagged_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df = 0.02, max_df=0.15, analyzer = \"word\",ngram_range=(1,2))\n",
    "cvz = vectorizer.fit_transform(df['stemmed_comments'])\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print(\"Total Vocab: {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('terms: {}'.format(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = []\n",
    "\n",
    "k_range = range(1,20)\n",
    "for n in tqdm.tqdm(k_range):\n",
    "    lda= LDA(random_state=44, n_components = n,max_iter=50, learning_method='online', learning_offset=60.)\n",
    "    lda.fit(cvz)\n",
    "    perplexity.append(lda.perplexity(cvz))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot(k_range, perplexity, color='b')\n",
    "ax1.set_xlabel('Cluster')\n",
    "ax1.set_ylabel('Perplexity', color='b')\n",
    "ax1.tick_params('y', colors='b')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 7# np.array(perplexity).argsort()[0]+1\n",
    "print(\"Number of clusters: {}\".format(n_components))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn  \n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "lda_model = LDA(random_state=44, n_components=n_components,max_iter=50,learning_method=\"online\", learning_offset=60.)\n",
    "transformed = lda_model.fit_transform(cvz)\n",
    "panel = pyLDAvis.sklearn.prepare(lda_model, cvz, vectorizer, mds='tsne',sort_topics=True, R=15)\n",
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info = panel.topic_info\n",
    "top10_terms = topic_info.groupby(['Category','Term'])['Freq'].sum().reset_index().sort_values(['Category', 'Freq'], ascending=[True, False]).groupby('Category').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10terms = list(top10_terms.groupby('Category')['Term'].apply(list).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_categories = pd.DataFrame()\n",
    "   \n",
    "prob = [max(proba) for proba in transformed]\n",
    "\n",
    "terms = [top10terms[cluster] for cluster in transformed.argsort()[:,::-1][:,0]]\n",
    "\n",
    "df['cluster'] = transformed.argsort()[:,::-1][:,0].astype(str)\n",
    "\n",
    "df['cluster_terms'] = terms\n",
    "\n",
    "df['cluster_terms'] = df['cluster_terms'].apply(lambda x: '\\n'.join(x))\n",
    "\n",
    "df['cluster_probability'] = prob\n",
    "\n",
    "dfg = df.groupby('cluster')['cluster'].count()\n",
    "\n",
    "\n",
    "for x,y in tqdm.tqdm(dfg.iteritems()):\n",
    "    temp_df = df[df['cluster']==x]\n",
    "    \n",
    "    try:\n",
    "        _vectorizer = CountVectorizer(min_df=0.02,max_df=0.15, ngram_range=(1,2), analyzer = \"word\")\n",
    "        _cvz = _vectorizer.fit_transform(temp_df.stemmed_comments)\n",
    "        _vocab = _vectorizer.get_feature_names()\n",
    "        _perplexity = []\n",
    "\n",
    "        for n in range(1,10):\n",
    "    \n",
    "            _lda= LDA(random_state=44, n_components = n, learning_method='online', learning_decay=0.6, learning_offset=10.)\n",
    "            _lda.fit(_cvz)\n",
    "            _perplexity.append(_lda.perplexity(_cvz))\n",
    "\n",
    "        _n_components = np.array(_perplexity).argsort()[0]+1    \n",
    "        print(\"{} subclusters for cluster {}\".format(_n_components, x))\n",
    "\n",
    "        if _n_components >1:\n",
    "            _lda_model = LDA(random_state=44, n_components=_n_components,learning_decay=0.6,learning_offset=10.,learning_method=\"online\")\n",
    "            _transformed = _lda_model.fit_transform(_cvz)\n",
    "            _prob = [max(proba) for proba in _transformed]\n",
    "\n",
    "            _cluster_words = []\n",
    "            for word_list in _lda_model.components_.argsort()[:,::-1][:,:10]:\n",
    "                _cluster_words.append(np.array(_vocab)[word_list.tolist()])\n",
    "\n",
    "            _terms = [_cluster_words[cluster] for cluster in _transformed.argsort()[:,::-1][:,0]]\n",
    "\n",
    "            temp_df['sub_cluster'] = _transformed.argsort()[:,::-1][:,0].astype(str)\n",
    "\n",
    "            temp_df['sub_cluster_terms'] = _terms\n",
    "\n",
    "            temp_df['sub_cluster_terms'] = temp_df['sub_cluster_terms'].apply(lambda x: '\\n'.join(x))\n",
    "\n",
    "            temp_df['sub_cluster_probability'] = _prob\n",
    "    except:\n",
    "        print(\"Not enough vocabulary for clustering.\")\n",
    "        \n",
    "\n",
    "    df_sub_categories = pd.concat([df_sub_categories, temp_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_categories.fillna('', inplace=True)\n",
    "df_sub_categories['cluster_sub_cluster'] = df_sub_categories[['cluster', 'sub_cluster']].apply( lambda x: str(x['cluster']+'.'+x['sub_cluster']+'0'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_categories[df_sub_categories['cluster']=='0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "timestamp = str(datetime.datetime.now()).split('.')[0]\n",
    "df_save = df_sub_categories.sort_values(['cluster_sub_cluster','cluster_probability', 'sub_cluster_probability'], ascending=[True,False, False])\n",
    "df_save.to_csv(get_output_file('clusters_mailessentials_'+timestamp+'.csv'),index=False,\n",
    "              columns=['id','title','ticket_type','merged_from','merged_into','description','comments_new','clean_comments','cluster','sub_cluster','cluster_terms','sub_cluster_terms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clustering",
   "language": "python",
   "name": "clustering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
